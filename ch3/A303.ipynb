{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86580010-f27c-4a4d-9d65-2f2dfb6bece2",
   "metadata": {},
   "source": [
    "# Making Use of Your Model\n",
    "\n",
    "We are going to use the saved model architecture in `final_model.py`, as well as a saved checkpoint in `final_model_checkpoint.pth` to augment a Flask API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2552034b-89c7-40ec-82b3-a53be37cf4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flask\n",
    "from flask import request\n",
    "import final_model\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "924a6f80-e856-410b-9a83-2633ffe2ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here final_model is a local python file and .Classifier is a class stored in that file\n",
    "def load_model_checkpoint(path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model = final_model.Classifier(checkpoint[\"input\"])\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    return model\n",
    "model = load_model_checkpoint(\"final_model_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a597190a-5867-4aef-b1aa-feffe4785983",
   "metadata": {},
   "source": [
    "Now we can create some input data to predict with. I'll be hardcoding the tensor but in reality this could be any data that has been converted to a tensor of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "526b1236-1b55-4c0a-8146-84485fac19a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[0.0606, 0.5000, 0.3333, 0.4828, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000,\n",
    "               0.4000, 0.1651, 0.0869, 0.0980, 0.1825, 0.1054, 0.2807, 0.0016, 0.0000,\n",
    "               0.0033, 0.0027, 0.0031, 0.0021]]).float()\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "492cf981-2a82-484b-84ee-75031b745d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1]])\n"
     ]
    }
   ],
   "source": [
    "# create a prediction using the example t\n",
    "pred = model(t)\n",
    "pred = torch.exp(pred)\n",
    "top_p, top_class_test = pred.topk(1, dim=1)\n",
    "print(top_class_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdac7bdc-1465-4b45-b80f-c1e84f31f569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1]])\n"
     ]
    }
   ],
   "source": [
    "# convert our model to torchscript\n",
    "traced_script = torch.jit.trace(model, t)\n",
    "traced_pred = traced_script(t)\n",
    "top_p, top_class_test = pred.topk(1, dim=1)\n",
    "print(top_class_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca02c0-5e3f-4f2f-8aa6-992e12fad651",
   "metadata": {},
   "source": [
    "Now we can create a basic flask app as before, as extend it to return a prediction based on passed in tensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "477a78f3-0e8a-48fb-a1cc-f3fcc839488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [29/Jul/2024 21:49:55] \"POST /prediction HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = flask.Flask(__name__)\n",
    "app.config[\"DEBUG\"] = True\n",
    "@app.route('/prediction', methods=['POST'])\n",
    "\n",
    "def prediction():\n",
    "    body = request.get_json()\n",
    "    example = torch.tensor(body['data']).float()\n",
    "    \n",
    "    pred = model(example)\n",
    "    pred = torch.exp(pred)\n",
    "    _, top_class_test = pred.topk(1, dim=1)\n",
    "    top_class_test = top_class_test.numpy()\n",
    "    \n",
    "    return {\"status\":\"ok\", \"result\":int(top_class_test[0][0])}\n",
    "\n",
    "app.run(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7347290-501c-48a9-93ee-9601826c0e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
